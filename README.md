# Group of One CS 410 Course Project (Fall 2023)

## Members 
- Alex Ho (ajho2)

## Project Description
While online learning platforms like Coursera have made education more accessible, they often lack intelligent features to help students effectively process information. Students usually encounter vast amounts of content in video lectures, making it challenging to review, identify key concepts, and pinpoint knowledge gaps. A hierarchical summarizer that presents lecture content at different levels of granularity can significantly aid students in their learning journey.

The proposed project aligns with the "Intelligent Learning Platform" track, as it leverages intelligent information retrieval, natural language processing, and machine learning. Specifically, the application of text-based information systems, retrieval models, and clustering, which are covered in our class, can offer personalized learning support by generating dynamic lecture summaries tailored to individual student needs.

---

# Documentation

## User Guide

### Prerequisites
- Python 3.8
- Libraries: `nltk`, `numpy`, `pandas`, `scipy`, `openai` (with conda, use `conda env create -f env.yml` to get required dependencies)
- An API key for OpenAI

### How to Use

1. **Prepare Your Environment:**
   Install the required Python libraries using `pip install nltk numpy pandas scipy openai`.

2. **Obtain an OpenAI API Key:**
   Get an API key from OpenAI and set it as an environment variable or pass it directly to the script.

3. **Running the Script:**
   - Input your transcript file in the designated input directory, e.g., `transcripts/your_transcript.txt`.
   - Execute the script with the input file path, output paths for the cleaned transcript, and the embeddings CSV.
   - Set the desired `relative_cutoff` for clustering.
   - Run the script to get the summaries for each cluster.

4. **Output:**
   The script will display the number of clusters formed and a summary for each cluster.

### Example
See the `demo.ipynb` notebook for example usage. 

## Developer Guide

### Code Structure
The tool is composed of several Python functions, each serving a specific purpose in the text analysis pipeline:

1. **`clean_transcript(file_path, output_path)`**
   - Cleans the input transcript by removing newlines and specific descriptors.
   - Inputs: Paths to the input and output files.
   - Outputs: A cleaned transcript file.

2. **`get_embedding(client, sentence)`**
   - Generates an embedding for a sentence using OpenAI's model.
   - Inputs: OpenAI client instance, sentence.
   - Outputs: Embedding array.

3. **`sentence_tokenizer_and_embed(file_path, output_csv_path, openai_api_key)`**
   - Tokenizes the transcript into sentences and generates embeddings.
   - Inputs: File paths, OpenAI API key.
   - Outputs: CSV file with sentences and embeddings.

4. **`compute_linkage_matrix(embeddings_csv_path)`**
   - Computes a linkage matrix from embeddings for hierarchical clustering.
   - Inputs: Path to the embeddings CSV.
   - Outputs: Linkage matrix.

5. **`form_clusters(linkage_matrix, relative_cutoff)`**
   - Forms clusters based on the linkage matrix and a cutoff value.
   - Inputs: Linkage matrix, cutoff value.
   - Outputs: Array of cluster labels.

6. **`summarize_cluster(sentences, cluster_indices, cluster_number, openai_api_key)`**
   - Generates a summary for a given cluster using GPT-3.5.
   - Inputs: Sentences, cluster indices, cluster number, OpenAI API key.
   - Outputs: Summary text.

### Implementation Details
- The tool utilizes the OpenAI API to generate sentence embeddings and summaries.
- Hierarchical clustering is used to group similar sentences.
- The `nltk` library is used for sentence tokenization.
- Summaries are generated by instructing GPT-3.5 to condense the main points of each cluster.

### Customization
- **API Key:** The OpenAI API key can be set as an environment variable or passed directly to the script.
- **Clustering Parameters:** The `relative_cutoff` for clustering can be adjusted based on the desired granularity.
- **Text Preprocessing:** The regex in `clean_transcript` can be modified to suit different transcript formats.
