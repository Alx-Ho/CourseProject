{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import pandas as pd\n",
    "\n",
    "from utils import clean_transcript, sentence_tokenizer_and_embed, compute_linkage_matrix, form_clusters, summarize_cluster\n",
    "\n",
    "openai_api_key='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas nltk scipy openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'transcripts/02_11-2-text-categorization-discriminative-classifier-part-2-optional.en.txt'\n",
    "cleaned_file = 'results/cleaned_lecture1.txt'  # Output path for the cleaned transcript\n",
    "\n",
    "clean_transcript(input_file, cleaned_file)\n",
    "\n",
    "output_csv = 'results/lecture1_embeddings.csv'  # File path to save embeddings and sentences\n",
    "\n",
    "sentence_tokenizer_and_embed(cleaned_file, output_csv, openai_api_key=openai_api_key)\n",
    "\n",
    "linkage_matrix = compute_linkage_matrix(output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters formed: 7\n"
     ]
    }
   ],
   "source": [
    "relative_cutoff = 0.5 # Example cutoff level\n",
    "clusters = form_clusters(linkage_matrix, relative_cutoff)\n",
    "\n",
    "# Display the number of clusters formed\n",
    "num_clusters = len(set(clusters))\n",
    "print(f'Number of clusters formed: {num_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 Summary:\n",
      "This lecture discusses discriminative classifiers for text categorization,\n",
      "emphasizing the importance of effective feature representation and the\n",
      "combination of different methods to improve performance and reduce mistakes. It\n",
      "also mentions the use of supervised machine learning, error analysis, feature\n",
      "selection, dimension reduction, and deep learning techniques for learning\n",
      "effective features. Additionally, it explores the challenges of obtaining\n",
      "training examples and suggests using low-quality or unlabeled data, as well as\n",
      "techniques like domain adaptation and transfer learning to handle different\n",
      "domains.\n",
      "\n",
      "Cluster 2 Summary:\n",
      "In this lecture, the Support Vector Machine (SVM) is introduced as a\n",
      "discriminative classifier that determines the best line based on support\n",
      "vectors. SVM is a simple optimization problem that seeks parameter values to\n",
      "optimize margins and training error, and can be modified to accommodate\n",
      "different scenarios, such as minimizing feature weights.\n",
      "\n",
      "Cluster 3 Summary:\n",
      "The lecture discusses the concept of a linear classifier or separator and how to\n",
      "choose the best one. The goal is to maximize the margin, which is the distance\n",
      "between the separator and the closest points from each class. The separator is\n",
      "determined by a few support vectors, and the margin is related to the magnitude\n",
      "of the weight. In some cases, a soft margin is used when the data points are not\n",
      "completely separable.\n",
      "\n",
      "Cluster 4 Summary:\n",
      "The lecture discusses the classification of documents into two categories using\n",
      "a classifier, where the sign of a function value determines the category. The\n",
      "goal is to minimize training errors and maximize the margin, but allowing some\n",
      "mistakes affects the generalization of the classifier. Once the weights and bias\n",
      "are obtained, the classifier can be used to classify new objects. The lecture\n",
      "also mentions aligning categories with training data and combining labeled\n",
      "examples with true training examples for improved categorization.\n",
      "\n",
      "Cluster 5 Summary:\n",
      "In this lecture, the speaker discusses a simple case of two-dimensional space\n",
      "and how it corresponds to a line. They examine the data instances on either side\n",
      "of the line and evaluate the value of a classifier for a particular data point.\n",
      "The speaker introduces the concept of support vectors and discusses the goal of\n",
      "capturing constraints in a unified way. They also mention the introduction of an\n",
      "extra variable and the pros and cons of different approaches. Overall, the idea\n",
      "of capturing constraints in a unified way is highlighted as a powerful concept.\n",
      "\n",
      "Cluster 6 Summary:\n",
      "Xi is a feature value that represents the error allowed for each data instance,\n",
      "and minimizing Xi is necessary to control the error.\n",
      "\n",
      "Cluster 7 Summary:\n",
      "The lecture discusses the decision boundary between different categories,\n",
      "represented by a line defined by three parameters (beta zero, beta one, and beta\n",
      "two). The line is determined by the coefficients (vector beta) and the goal is\n",
      "to find values for the weights (w) and bias constant (b) to determine the\n",
      "separator. The classifier is a linear separator with weights for all features,\n",
      "and the objective is to minimize the weights and errors while maximizing the\n",
      "margin. The parameter C controls the trade-off between minimizing errors and\n",
      "maximizing the margin, and its value needs to be carefully optimized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load sentences\n",
    "df = pd.read_csv(output_csv, header=None)\n",
    "sentences = df[0].tolist()\n",
    "\n",
    "# Define the width for wrapping text\n",
    "wrap_width = 80\n",
    "\n",
    "# Summarize each cluster\n",
    "summaries = []\n",
    "for i in range(1, num_clusters + 1):\n",
    "    summary = summarize_cluster(sentences, clusters, i, openai_api_key=openai_api_key)\n",
    "    summaries.append(summary)\n",
    "    formatted_summary = textwrap.fill(summary, width=wrap_width)\n",
    "    print(f'Cluster {i} Summary:\\n{formatted_summary}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
