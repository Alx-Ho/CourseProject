{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to either create a conda environment with `env.yml` or pip install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import pandas as pd\n",
    "\n",
    "from utils import clean_transcript, sentence_tokenizer_and_embed, compute_linkage_matrix, form_clusters, summarize_cluster\n",
    "\n",
    "openai_api_key='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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You don't need to change anything in this notebook to run the example, but you may use your own lecture transcript by changing the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'transcripts/02_11-2-text-categorization-discriminative-classifier-part-2-optional.en.txt' # Modify the path to wherever your transcript is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file = 'results/cleaned_lecture1.txt'  # Output path for the cleaned transcript\n",
    "\n",
    "clean_transcript(input_file, cleaned_file)\n",
    "\n",
    "output_csv = 'results/lecture1_embeddings.csv'  # File path to save embeddings and sentences\n",
    "\n",
    "sentence_tokenizer_and_embed(cleaned_file, output_csv, openai_api_key=openai_api_key)\n",
    "\n",
    "linkage_matrix = compute_linkage_matrix(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters formed: 6\n"
     ]
    }
   ],
   "source": [
    "relative_cutoff = 0.5 # Example cutoff level\n",
    "clusters = form_clusters(linkage_matrix, relative_cutoff)\n",
    "\n",
    "# Display the number of clusters formed\n",
    "num_clusters = len(set(clusters))\n",
    "print(f'Number of clusters formed: {num_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 Summary:\n",
      "This lecture discusses discriminative classifiers for text categorization,\n",
      "emphasizing the importance of effective feature representation and the\n",
      "combination of different methods to improve performance and reduce mistakes. It\n",
      "also mentions the use of supervised machine learning, the need for domain\n",
      "knowledge in feature design, error analysis for insights, and techniques like\n",
      "feature selection and dimension reduction. Additionally, it mentions the\n",
      "potential of deep learning for learning representations in text processing, the\n",
      "challenges of obtaining training examples, and the use of pseudo training\n",
      "examples and semi-supervised learning techniques. Finally, it suggests further\n",
      "reading for more details on the covered methods.\n",
      "\n",
      "Cluster 2 Summary:\n",
      "In this lecture, the Support Vector Machine (SVM) is introduced as a\n",
      "discriminative classifier that determines the best line based on support\n",
      "vectors. SVM is a simple optimization problem that seeks parameter values to\n",
      "optimize margins and training error, and can be modified to accommodate\n",
      "different scenarios, such as minimizing feature weights.\n",
      "\n",
      "Cluster 3 Summary:\n",
      "The lecture discusses the concept of a linear classifier or separator and how to\n",
      "choose the best one. The goal is to maximize the margin, which is the distance\n",
      "between the separator and the closest points from each class. The separator is\n",
      "determined by a few support vectors, and the margin is related to the magnitude\n",
      "of the weight. In some cases, a soft margin is used when the data points are not\n",
      "completely separable.\n",
      "\n",
      "Cluster 4 Summary:\n",
      "The lecture discusses the classification of documents into two categories using\n",
      "a classifier, where the sign of a function value determines the category. The\n",
      "goal is to minimize training errors and maximize the margin, but allowing some\n",
      "mistakes affects the generalization of the classifier. Once the weights and bias\n",
      "are obtained, the classifier can be used to classify new objects. The lecture\n",
      "also mentions aligning categories with training data and combining labeled\n",
      "examples with true training examples for improved categorization.\n",
      "\n",
      "Cluster 5 Summary:\n",
      "The lecture discusses the assumption and setup of a positive feature value, the\n",
      "goal of minimizing errors, and the importance of controlling and minimizing the\n",
      "error value Xi i in order to optimize training. It concludes that there is no\n",
      "clear winner in terms of the best approach, but emphasizes the power of the\n",
      "discussed idea.\n",
      "\n",
      "Cluster 6 Summary:\n",
      "The lecture discusses the decision boundary in a two-dimensional space and\n",
      "introduces the concept of a linear separator determined by weights and a bias\n",
      "constant. It explains the use of support vectors and the formulation of the\n",
      "classifier as a linear separator. The lecture then introduces the optimization\n",
      "problem of maximizing the margin while minimizing errors, with the trade-off\n",
      "controlled by a parameter C. The lecture concludes by mentioning the need to\n",
      "carefully choose the value of C and the possibility of using cross-validation\n",
      "for optimization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load sentences\n",
    "df = pd.read_csv(output_csv, header=None)\n",
    "sentences = df[0].tolist()\n",
    "\n",
    "# Define the width for wrapping text\n",
    "wrap_width = 80\n",
    "\n",
    "# Summarize each cluster\n",
    "summaries = []\n",
    "for i in range(1, num_clusters + 1):\n",
    "    summary = summarize_cluster(sentences, clusters, i, openai_api_key=openai_api_key)\n",
    "    summaries.append(summary)\n",
    "    formatted_summary = textwrap.fill(summary, width=wrap_width)\n",
    "    print(f'Cluster {i} Summary:\\n{formatted_summary}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
